# ğŸ’¼ Salary Prediction System

An intelligent **Machine Learning web application** that predicts employee salaries based on inputs like age, department, experience, and education.  
It also provides **AI-generated insights** using **Google Gemini** (or any other LLM API key configured by the user).

---

## ğŸš€ Live Demo
ğŸ‘‰ **[View on Render](https://salary-prediction-gi9d.onrender.com/)**

---

## ğŸ§  Project Overview
The project allows both **manual** and **bulk (CSV)** salary predictions, supported by:
- ğŸ’¡ AI-powered salary insights using API  
- ğŸ“„ Bulk Upload option for CSV files  
- ğŸ§® Trained ML Model using Scikit-learn  
- ğŸŒ Interactive Bootstrap-based web interface  
- â˜ï¸ Fully deployed on Render  

---

## ğŸ§© Features
- ğŸ’¬ **AI Insight Generation** using Gemini  
- ğŸ§  **ML Model** trained for salary prediction  
- ğŸ“‚ **Bulk CSV Upload** for multi-record predictions  
- âš™ï¸ **Flask Web Framework** for backend  
- ğŸ’» **Bootstrap 5 UI** for responsive design  
- ğŸŒ **Render Deployment** for global access  

---

## ğŸ› ï¸ Tech Stack

| Category | Tools / Libraries |
|-----------|------------------|
| **Frontend** | HTML5, CSS3, Bootstrap 5 |
| **Backend** | Python, Flask |
| **Machine Learning** | Scikit-learn, Pandas, NumPy |
| **AI Integration** | Google Gemini API (`google-generativeai`) |
| **Deployment** | Render |
| **Version Control** | Git, GitHub |

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/yourusername/salary-prediction-system.git
cd salary-prediction-system

### 2ï¸âƒ£ Create a Virtual Environment
```bash
python -m venv venv
venv\Scripts\activate      # On Windows
# or
source venv/bin/activate   # On macOS/Linux

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt

## ğŸ”‘ Setting Up the API Key (LLM AI Insight)
The project uses Google Gemini API for generating AI insights about predicted salaries.

### â¤ Option 1: Using .env file (Recommended)
- Create a file named .env in your project root directory.
- Add your Gemini API key inside .env as follows:
```ini
GEMINI_API_KEY=your_actual_api_key_here
- Ensure your app.py contains this code:
```python
from dotenv import load_dotenv
import os, google.generativeai as genai

load_dotenv()  # Load environment variables

def llm_call(prediction, params):
    try:
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        model = genai.GenerativeModel("gemini-2.5-flash")
        # your logic for AI insight generation here
    except Exception as e:
        print("LLM Error:", e)

### â¤ Option 1: Using .env file (Recommended)
If you are using a different AI provider, follow these steps:
- Replace your .env content completely with your custom key, for example:
```ini
OPENAI_API_KEY=your_openai_key_here
- Then modify your app.py code:
```python
import os, openai
from dotenv import load_dotenv

load_dotenv()

def llm_call(prediction, params):
    try:
        openai.api_key = os.getenv("OPENAI_API_KEY")
        # Replace with your model and logic
        response = openai.Completion.create(
            model="gpt-4-turbo",
            prompt=f"Explain this salary prediction: {prediction}, {params}"
        )
        return response.choices[0].text
    except Exception as e:
        print("LLM Error:", e)
### âš ï¸ Important:
Always match your .env variable name and the key name in your code.

## â–¶ï¸ Running the Application

### For local testing, use:
```python
if __name__ == '__main__':
    app.run(debug=True)

### For production (Render deployment), uncomment the serve() line:
Uncomment the serve() line in app.py:
```python
if __name__ == '__main__':
    # serve(app, host='0.0.0.0', port=8080)  # For production (Render)
    app.run(debug=True)  # For local testing only

## ğŸ“š Future Enhancements

- ğŸ” **Add user authentication**  
  Implement login and registration functionality for secure user access.

- ğŸ“Š **Data visualization for salary trends**  
  Integrate interactive charts to show salary distributions and trends.

- ğŸ§  **Multi-model AI support (Gemini / OpenAI / Claude)**  
  Allow users to choose between multiple LLM providers for generating insights.

- ğŸ—„ï¸ **Database integration for history tracking**  
  Store previous predictions and AI insights for future reference.